# Project Overview

## Introduction

This project is developed to create an API that automates the classification of products into "Direct" and "Indirect" categories based on their descriptions and the commercial activity code (CNAE) of the company. It utilizes advanced machine learning models, specifically Vertex AI's Text Generation Model, to interpret product descriptions and classify them accordingly. This automation aims to streamline inventory management and procurement processes for businesses across various sectors.

## Objectives

- **Automate Classification Process:** To reduce manual intervention and improve the accuracy of classifying products as Direct or Indirect.
- **Leverage Vertex AI Models:** To utilize cutting-edge AI for understanding and categorizing product descriptions.
- **Improve Data Handling:** To efficiently manage product data, including implementing caching for faster query responses.
- **Robust Logging:** To maintain detailed logs for monitoring application performance and facilitating troubleshooting.

## Project Structure

Below is the tree view of the project files, providing a clear overview of its structure and the purpose of each file:

project/  
│  
├── app/  
│ ├── application.py # Main FastAPI application, defines API endpoints and integrates with Vertex AI.  
│ └── config.py # Configuration settings, including credentials and model parameters.  
│  
├── data/  
│ ├── api.log # Application log file for recording runtime information.  
│ └── cnaes_list.py # Contains the list of CNAE codes and their descriptions.  
│  
├── tests/  
│ └── test_application.py # Contains unit tests for the application to ensure reliability and correctness.   
│  
├── locustfile.py # File configuration for application's load tests.     
│  
├── requirements.txt # Lists all the Python package dependencies required to run the project.  
│  
└── README.md # Project documentation and overview.  


### File Descriptions

- **application.py:** The backbone of the FastAPI application, this file includes endpoint definitions for health checks and the main product classification functionality. It sets up the Vertex AI model, caching, and logging configurations.
- **config.py:** Stores all the necessary configuration variables such as paths to credential files, Vertex AI project details, and model names. It centralizes the configuration to facilitate easy updates and maintenance.
- **api.log:** A log file where the application records its operational logs. It's crucial for monitoring the app's health and debugging issues.
- **cnaes_list.py:** A Python module that provides a list of CNAE codes along with their descriptions. This list is essential for the classification logic based on commercial activities.


This structure is designed to ensure clarity, maintainability, and ease of navigation throughout the project, facilitating efficient development and future enhancements.


## Running the Application

After setting up the environment and installing the dependencies, you can run the application using Uvicorn. Uvicorn is an ASGI server that allows you to run FastAPI applications with high performance.

- **Run with Uvicorn Directly**

  You can start the application by running Uvicorn directly from the command line, specifying the application instance:

```
uvicorn app.definition:app_definition --host 127.0.0.1 --port 8080
```

or executing the following command in terminal:

```
python -m app.__main__
```


Once the application is running locally, you can test its endpoints by navigating to `http://127.0.0.1:8080/docs` in your web browser. This URL opens the FastAPI autogenerated documentation page, which provides an interactive API documentation interface. Here, you'll see a list of all the available endpoints, including their HTTP methods and descriptions. You can expand each endpoint to view its parameters, request body schema, and response model. To test an endpoint, simply click on it, fill in any required parameters or request body, and hit the "Try it out" button. The interface will execute the request against your local application and display the response, allowing you to easily test and debug the API's functionality.

## Importance of Unit Testing and Execution Guide

Unit testing plays a crucial role in software development, especially in projects involving complex logic and integrations, such as this one. By isolating and testing individual units of code, developers can verify that each part functions correctly, leading to a more reliable and maintainable application overall. Here are some key benefits of unit testing:

- **Early Bug Detection:** Unit tests help catch bugs early in the development cycle, saving time and effort in the later stages.
- **Facilitates Refactoring:** With a suite of tests in place, developers can refactor code with confidence, ensuring that changes do not break existing functionality.
- **Improves Code Quality:** Writing tests encourages developers to write more modular, efficient code, which improves the overall quality of the application.
- **Documentation:** Unit tests serve as a form of documentation, showing how the code is supposed to work and how to use the APIs.

### Running Unit Tests

To run the unit tests for this application, ensure you have the project dependencies installed and are in the project's root directory. The tests are located in the `tests/` directory and can be executed using the following command:

```
pytest
```


This command will discover and run all the test files in the project that match the pattern `test_*.py` or `*_test.py`, providing a report on the success or failure of each test.

If you wish to run a specific test file or a subset of tests, you can specify the file or directory as an argument:

```
pytest tests/test_application.py
```

This will run all the tests defined in `test_application.py`.

### Conclusion

Unit testing ensures a bug-free and maintainable application. Regularly running these tests during development can significantly improve the quality and reliability of the application.

## Use of cache

In this project, caching is implemented to enhance the efficiency and performance of the application by temporarily storing results of expensive function calls or frequently accessed data. When a product description is classified, the result is stored in a cache with a unique key, typically derived from the product description or other identifying information. Subsequent requests for the same classification check the cache first; if the result is found, it is returned immediately, bypassing the need to reprocess the classification through the Vertex AI model. This reduces response times and decreases the load on the system, especially for repeated queries of the same or similar product descriptions. The cache is managed to ensure that data remains up-to-date by setting appropriate expiration times or invalidating entries when underlying data changes.


## Performance Improvements in classify_product function

1. **JSON Parsing Elimination**
   - Directly uses `data_json` as a dictionary, avoiding unnecessary parsing and reducing computational overhead.

2. **CNAE Number Handling**
   - Simplifies `cnae_number` validation and padding with `.zfill(7)`, minimizing string operations and conditional checks.

3. **Model and Parameters Selection**
   - Condenses model and parameter selection into a single line using tuple assignment, enhancing efficiency and readability.

4. **Batch Processing Optimization**
   - Streamlines the approach for slicing `item_description` for batch processing, improving efficiency and readability.

5. **Item Cleaning with List Comprehension**
   - Maintains the use of list comprehension for cleaning items, which is faster than equivalent for-loops for simple transformations.

6. **Reduced Redundancy in Error Handling**
   - Implements an early return for missing required fields, allowing the function to "fail fast" and save computational resources.

7. **Efficient Final JSON Construction**
   - Utilizes `extend()` for appending multiple items to the final JSON list, which is more efficient than appending inside a loop.

These enhancements lead to a codebase that is not only more efficient but also easier to read and maintain.

## Load Testing with Locust

For assessing the performance of our API under various loads, a test using the Locust Python library has been conducted. This testing aimed to simulate real-world scenarios and evaluate how our API handles concurrent user requests. To reproduce this test locally it is necessary to run the following command in terminal (it is necessary to maitain the application running):

```
locust --host http://127.0.0.1:8080
```

### Test Configuration using palm-2 (application.py without cache):
- **Number of Users:** 10
- **Ramp-Up:** 1 user per second
- **Number of Workers:** 4 (Machine resources)
- **Cache Usage:** Not utilized
- **Requests per Second:** Approximately 5.5 on average
- **Average Response Time:** 1.5 seconds

### Results:

#### User Load vs. Time:

![Alt text](<data/pictures/Screenshot from 2024-02-19 22-25-08.png>)

#### Requests per Second vs. Time:

![Alt text](<data/pictures/Screenshot from 2024-02-19 22-22-43.png>)

#### Average Response Time vs. Time:

![Alt text](<data/pictures/Screenshot from 2024-02-19 22-23-58.png>)

### Test Configuration using gpt-3.5-turbo-16k (application_gpt.py):
- **Number of Users:** 10
- **Ramp-Up:** 1 user per second
- **Number of Workers:** 4 (Machine resources)
- **Cache Usage:** Not utilized
- **Requests per Second:** Approximately 3.5 on average
- **Average Response Time:** 2.5 seconds

### Results:

#### User Load vs. Time:

![Alt text](<data/pictures/Screenshot from 2024-02-22 21-27-14.png>)

#### Requests per Second vs. Time:

![Alt text](<data/pictures/Screenshot from 2024-02-22 21-26-55.png>)

#### Average Response Time vs. Time:

![Alt text](<data/pictures/Screenshot from 2024-02-22 21-27-04.png>)


#### Conclusion:
In conclusion, based on the load testing results using the same computational resources, the palm-2 model showcased superior performance in terms of throughput and response time compared to the gpt-3.5-turbo-16k model. This indicates that palm-2 may be more suitable for handling concurrent user requests efficiently in this application scenario.
